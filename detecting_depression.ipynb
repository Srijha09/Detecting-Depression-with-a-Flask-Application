{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1595524593312",
   "display_name": "Python 3.7.7 64-bit ('tensorflow_env': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import itertools\n",
    "from scipy import stats\n",
    "from ast import literal_eval\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, LSTM, Input, Activation, GlobalAveragePooling1D, Flatten, Concatenate, Conv1D, MaxPooling1D,Bidirectional,TimeDistributed,Reshape,Conv2D,MaxPool2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adagrad, Adam\n",
    "from tensorflow.keras.preprocessing.text import one_hot, text_to_word_sequence, Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.utils import plot_model\n",
    "\n",
    "import fnmatch\n",
    "\n",
    "import warnings\n",
    "\n",
    "import string\n",
    "from pathlib import Path\n",
    "from random import shuffle\n",
    "from ast import literal_eval\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package stopwords to C:\\Users\\Srijha\n[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to C:\\Users\\Srijha\n[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package punkt to C:\\Users\\Srijha\n[nltk_data]     Kalyan\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import tensorflow.keras.utils\n",
    "from tensorflow.keras import utils as np_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOWS_SIZE = 10\n",
    "labels=['none','mild','moderate','moderately severe', 'severe']\n",
    "num_classes = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcripts_to_dataframe(directory):\n",
    "    rows_list = []\n",
    "        \n",
    "    filenames = os.listdir(directory)\n",
    "    \n",
    "    if \".DS_Store\" in filenames:\n",
    "        filenames.remove(\".DS_Store\")\n",
    "        \n",
    "    for filename in filenames:\n",
    "        transcript_path = os.path.join(directory, filename)\n",
    "        transcript = pd.read_csv(transcript_path, sep='\\t')\n",
    "        m = re.search(\"(\\d{3})_TRANSCRIPT.csv\", filename)\n",
    "        if m:\n",
    "            person_id = m.group(1)\n",
    "            p = {}\n",
    "            question = \"\"\n",
    "            answer = \"\"\n",
    "            lines = len(transcript)\n",
    "            for i in range(0, lines):\n",
    "                row = transcript.iloc[i]\n",
    "                if (row[\"speaker\"] == \"Ellie\") or (i == lines - 1):\n",
    "                    p[\"personId\"] = person_id\n",
    "                    if \"(\" in str(question):\n",
    "                        question = question[question.index(\"(\") + 1:question.index(\")\")]\n",
    "                    p[\"question\"] = question\n",
    "                    p[\"answer\"] = answer\n",
    "                    if question != \"\":\n",
    "                        rows_list.append(p)\n",
    "                    p = {}\n",
    "                    answer = \"\"\n",
    "                    question = row[\"value\"]\n",
    "                else:\n",
    "                    answer = str(answer) + \" \" + str(row[\"value\"])\n",
    "\n",
    "    all_participants = pd.DataFrame(rows_list, columns=['personId', 'question', 'answer'])\n",
    "    all_participants.to_csv(directory + 'all.csv', sep=',')\n",
    "    print(\"File was created\")\n",
    "    return all_participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File was created\n"
    }
   ],
   "source": [
    "#loading the data\n",
    "data_path = \"transcripts/\"\n",
    "all_participants = transcripts_to_dataframe(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   personId                                           question  \\\n0       300            hi i'm ellie thanks for coming in today   \n1       300  i was created to talk to people in a safe and ...   \n2       300  think of me as a friend i don't judge i can't ...   \n3       300  i'm here to learn about people and would love ...   \n4       300  i'll ask a few questions to get us started and...   \n5       300                            how are you doing today   \n6       300                                        that's good   \n7       300                      where are you from originally   \n8       300                                             really   \n9       300                              why'd you move to l_a   \n10      300                                how do you like l_a   \n11      300     what are some things you really like about l_a   \n12      300  how easy was it for you to get used to living ...   \n13      300  what are some things you don't really like abo...   \n14      300                                                mhm   \n15      300                                               okay   \n16      300                         what'd you study at school   \n17      300                                               cool   \n18      300                           are you still doing that   \n19      300                              what's your dream job   \n\n                                               answer  \n0                                                      \n1                                                      \n2                                                      \n3                                                      \n4                                                      \n5                                                good  \n6                                                      \n7                                     atlanta georgia  \n8                                                      \n9                      um my parents are from here um  \n10                                          i love it  \n11   i like the weather i like the opportunities u...  \n12                  um it took a minute somewhat easy  \n13                                         congestion  \n14                                          that's it  \n15                                                     \n16           um i took up business and administration  \n17                                                     \n18   uh yeah i am here and there i'm on a break ri...  \n19             uh probably to open up my own business  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>personId</th>\n      <th>question</th>\n      <th>answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300</td>\n      <td>hi i'm ellie thanks for coming in today</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>300</td>\n      <td>i was created to talk to people in a safe and ...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>think of me as a friend i don't judge i can't ...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>300</td>\n      <td>i'm here to learn about people and would love ...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300</td>\n      <td>i'll ask a few questions to get us started and...</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>300</td>\n      <td>how are you doing today</td>\n      <td>good</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>300</td>\n      <td>that's good</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>300</td>\n      <td>where are you from originally</td>\n      <td>atlanta georgia</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>300</td>\n      <td>really</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>300</td>\n      <td>why'd you move to l_a</td>\n      <td>um my parents are from here um</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>300</td>\n      <td>how do you like l_a</td>\n      <td>i love it</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>300</td>\n      <td>what are some things you really like about l_a</td>\n      <td>i like the weather i like the opportunities u...</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>300</td>\n      <td>how easy was it for you to get used to living ...</td>\n      <td>um it took a minute somewhat easy</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>300</td>\n      <td>what are some things you don't really like abo...</td>\n      <td>congestion</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>300</td>\n      <td>mhm</td>\n      <td>that's it</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>300</td>\n      <td>okay</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>300</td>\n      <td>what'd you study at school</td>\n      <td>um i took up business and administration</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>300</td>\n      <td>cool</td>\n      <td></td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>300</td>\n      <td>are you still doing that</td>\n      <td>uh yeah i am here and there i'm on a break ri...</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>300</td>\n      <td>what's your dream job</td>\n      <td>uh probably to open up my own business</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "all_participants.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "\n",
    "def text_to_wordlist(text, remove_stopwords=True, stem_words=False):    \n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = stopwords.words(\"english\")\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text if not w in stops ]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    else:\n",
    "        text = [wordnet_lemmatizer.lemmatize(w) for w in text]\n",
    "        text = [w for w in text if w != \"nan\" ]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    \n",
    "    text = re.sub(r\"\\<\", \" \", text)\n",
    "    text = re.sub(r\"\\>\", \" \", text)\n",
    "    \n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a corpus with the words from the answers without stopwords given by the patients\n",
    "all_participants_mix = all_participants.copy() # However, if you need the original list unchanged when the new list is modified, you can use copy() method. This is called shallow copy.\n",
    "all_participants_mix[\"answer\"] = all_participants_mix.apply(lambda row: text_to_wordlist(row.answer).split(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [w for w in all_participants_mix['answer'].tolist()]\n",
    "words = set(itertools.chain(*words)) #chain('ABC', 'DEF') --> A B C D E F\n",
    "vocab_size = len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   personId                                           question  \\\n0       300            hi i'm ellie thanks for coming in today   \n1       300  i was created to talk to people in a safe and ...   \n2       300  think of me as a friend i don't judge i can't ...   \n3       300  i'm here to learn about people and would love ...   \n4       300  i'll ask a few questions to get us started and...   \n5       300                            how are you doing today   \n6       300                                        that's good   \n7       300                      where are you from originally   \n8       300                                             really   \n9       300                              why'd you move to l_a   \n10      300                                how do you like l_a   \n11      300     what are some things you really like about l_a   \n12      300  how easy was it for you to get used to living ...   \n13      300  what are some things you don't really like abo...   \n14      300                                                mhm   \n\n                                         answer                 t_answer  \n0                                            []                       []  \n1                                            []                       []  \n2                                            []                       []  \n3                                            []                       []  \n4                                            []                       []  \n5                                        [good]                     [16]  \n6                                            []                       []  \n7                            [atlanta, georgia]             [1634, 1997]  \n8                                            []                       []  \n9                              [um, parent, um]              [1, 131, 1]  \n10                                       [love]                     [63]  \n11  [like, weather, like, opportunity, um, yes]  [5, 142, 5, 334, 1, 39]  \n12           [um, took, minute, somewhat, easy]  [1, 154, 527, 608, 100]  \n13                                 [congestion]                   [1998]  \n14                                       [that]                     [20]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>personId</th>\n      <th>question</th>\n      <th>answer</th>\n      <th>t_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>300</td>\n      <td>hi i'm ellie thanks for coming in today</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>300</td>\n      <td>i was created to talk to people in a safe and ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>300</td>\n      <td>think of me as a friend i don't judge i can't ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>300</td>\n      <td>i'm here to learn about people and would love ...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>300</td>\n      <td>i'll ask a few questions to get us started and...</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>300</td>\n      <td>how are you doing today</td>\n      <td>[good]</td>\n      <td>[16]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>300</td>\n      <td>that's good</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>300</td>\n      <td>where are you from originally</td>\n      <td>[atlanta, georgia]</td>\n      <td>[1634, 1997]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>300</td>\n      <td>really</td>\n      <td>[]</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>300</td>\n      <td>why'd you move to l_a</td>\n      <td>[um, parent, um]</td>\n      <td>[1, 131, 1]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>300</td>\n      <td>how do you like l_a</td>\n      <td>[love]</td>\n      <td>[63]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>300</td>\n      <td>what are some things you really like about l_a</td>\n      <td>[like, weather, like, opportunity, um, yes]</td>\n      <td>[5, 142, 5, 334, 1, 39]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>300</td>\n      <td>how easy was it for you to get used to living ...</td>\n      <td>[um, took, minute, somewhat, easy]</td>\n      <td>[1, 154, 527, 608, 100]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>300</td>\n      <td>what are some things you don't really like abo...</td>\n      <td>[congestion]</td>\n      <td>[1998]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>300</td>\n      <td>mhm</td>\n      <td>[that]</td>\n      <td>[20]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "windows_size = WINDOWS_SIZE\n",
    "tokenizer = Tokenizer(num_words=vocab_size)\n",
    "tokenizer.fit_on_texts(all_participants_mix['answer']) # fit_on_texts creates the vocabulary index based on word frequency.\n",
    "#The cat sat on the mat.\" It will create a dictionary s.t. word_index[\"the\"] = 1; word_index[\"cat\"] = 2 it is word -> index dictionary so every word gets a unique integer value. 0 is reserved for padding. So lower integer means more frequent word \n",
    "\n",
    "tokenizer.fit_on_sequences(all_participants_mix['answer']) #texts_to_sequences Transforms each text in texts to a sequence of integers. So it basically takes each word in the text and replaces it with its corresponding integer value from the word_index dictionary.\n",
    "\n",
    "all_participants_mix['t_answer'] = tokenizer.texts_to_sequences(all_participants_mix['answer'])\n",
    "all_participants_mix.head(15)\n",
    "\n",
    "#   why are the output as numbers when text_to_sequences is called?\n",
    "# the Tokenizer stores everything in the word_index during fit_on_texts. Then, when calling the texts_to_sequences method, only the top num_words are considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "    Unnamed: 0  personId                                             answer  \\\n0            0       300  ['good', 'atlanta', 'georgia', 'um', 'parent',...   \n1            1       300  ['atlanta', 'georgia', 'um', 'parent', 'um', '...   \n2            2       300  ['georgia', 'um', 'parent', 'um', 'love', 'lik...   \n3            3       300  ['um', 'parent', 'um', 'love', 'like', 'weathe...   \n4            4       300  ['parent', 'um', 'love', 'like', 'weather', 'l...   \n5            5       300  ['um', 'love', 'like', 'weather', 'like', 'opp...   \n6            6       300  ['love', 'like', 'weather', 'like', 'opportuni...   \n7            7       300  ['like', 'weather', 'like', 'opportunity', 'um...   \n8            8       300  ['weather', 'like', 'opportunity', 'um', 'yes'...   \n9            9       300  ['like', 'opportunity', 'um', 'yes', 'um', 'to...   \n10          10       300  ['opportunity', 'um', 'yes', 'um', 'took', 'mi...   \n11          11       300  ['um', 'yes', 'um', 'took', 'minute', 'somewha...   \n12          12       300  ['yes', 'um', 'took', 'minute', 'somewhat', 'e...   \n13          13       300  ['um', 'took', 'minute', 'somewhat', 'easy', '...   \n14          14       300  ['took', 'minute', 'somewhat', 'easy', 'conges...   \n\n                                             t_answer  \n0          [16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]  \n1         [1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]  \n2            [1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]  \n3              [1, 131, 1, 63, 5, 142, 5, 334, 1, 39]  \n4              [131, 1, 63, 5, 142, 5, 334, 1, 39, 1]  \n5              [1, 63, 5, 142, 5, 334, 1, 39, 1, 154]  \n6            [63, 5, 142, 5, 334, 1, 39, 1, 154, 527]  \n7           [5, 142, 5, 334, 1, 39, 1, 154, 527, 608]  \n8         [142, 5, 334, 1, 39, 1, 154, 527, 608, 100]  \n9        [5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]  \n10      [334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]  \n11        [1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]  \n12      [39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]  \n13     [1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]  \n14  [154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>personId</th>\n      <th>answer</th>\n      <th>t_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>300</td>\n      <td>['good', 'atlanta', 'georgia', 'um', 'parent',...</td>\n      <td>[16, 1634, 1997, 1, 131, 1, 63, 5, 142, 5]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>300</td>\n      <td>['atlanta', 'georgia', 'um', 'parent', 'um', '...</td>\n      <td>[1634, 1997, 1, 131, 1, 63, 5, 142, 5, 334]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>300</td>\n      <td>['georgia', 'um', 'parent', 'um', 'love', 'lik...</td>\n      <td>[1997, 1, 131, 1, 63, 5, 142, 5, 334, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>300</td>\n      <td>['um', 'parent', 'um', 'love', 'like', 'weathe...</td>\n      <td>[1, 131, 1, 63, 5, 142, 5, 334, 1, 39]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>300</td>\n      <td>['parent', 'um', 'love', 'like', 'weather', 'l...</td>\n      <td>[131, 1, 63, 5, 142, 5, 334, 1, 39, 1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>5</td>\n      <td>300</td>\n      <td>['um', 'love', 'like', 'weather', 'like', 'opp...</td>\n      <td>[1, 63, 5, 142, 5, 334, 1, 39, 1, 154]</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>6</td>\n      <td>300</td>\n      <td>['love', 'like', 'weather', 'like', 'opportuni...</td>\n      <td>[63, 5, 142, 5, 334, 1, 39, 1, 154, 527]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>7</td>\n      <td>300</td>\n      <td>['like', 'weather', 'like', 'opportunity', 'um...</td>\n      <td>[5, 142, 5, 334, 1, 39, 1, 154, 527, 608]</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>8</td>\n      <td>300</td>\n      <td>['weather', 'like', 'opportunity', 'um', 'yes'...</td>\n      <td>[142, 5, 334, 1, 39, 1, 154, 527, 608, 100]</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9</td>\n      <td>300</td>\n      <td>['like', 'opportunity', 'um', 'yes', 'um', 'to...</td>\n      <td>[5, 334, 1, 39, 1, 154, 527, 608, 100, 1998]</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>10</td>\n      <td>300</td>\n      <td>['opportunity', 'um', 'yes', 'um', 'took', 'mi...</td>\n      <td>[334, 1, 39, 1, 154, 527, 608, 100, 1998, 20]</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>11</td>\n      <td>300</td>\n      <td>['um', 'yes', 'um', 'took', 'minute', 'somewha...</td>\n      <td>[1, 39, 1, 154, 527, 608, 100, 1998, 20, 1]</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>12</td>\n      <td>300</td>\n      <td>['yes', 'um', 'took', 'minute', 'somewhat', 'e...</td>\n      <td>[39, 1, 154, 527, 608, 100, 1998, 20, 1, 154]</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>13</td>\n      <td>300</td>\n      <td>['um', 'took', 'minute', 'somewhat', 'easy', '...</td>\n      <td>[1, 154, 527, 608, 100, 1998, 20, 1, 154, 188]</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>14</td>\n      <td>300</td>\n      <td>['took', 'minute', 'somewhat', 'easy', 'conges...</td>\n      <td>[154, 527, 608, 100, 1998, 20, 1, 154, 188, 1376]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "phrases_lp = pd.read_csv('phrases_lp.csv', sep='\\t', converters={\"t_answer\": literal_eval}) \n",
    "phrases_lp.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "def load_avec_dataset_file(path,score_column):\n",
    "    ds = pd.read_csv(path, sep=',')\n",
    "    ds['level'] = pd.cut(ds[score_column], bins=[-1,0,5,10,15,25], labels=[0,1,2,3,4])  #cut function used to segregate array into bins 5 levels - 'none','mild','moderate','moderately severe', 'severe'\n",
    "    ds['PHQ8_Score'] = ds[score_column]\n",
    "    ds['cat_level'] = to_categorical(ds['level'], num_classes).tolist() #categorical levels \n",
    "    ds = ds[['Participant_ID', 'level', 'cat_level', 'PHQ8_Score','Gender']] \n",
    "    ds = ds.astype({\"Participant_ID\": float, \"level\": int, 'PHQ8_Score': int})\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Size: train= 107, dev= 35, test=47\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Participant_ID  level                  cat_level  PHQ8_Score  Gender\n0           303.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0\n1           304.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           6       0\n2           305.0      2  [0.0, 0.0, 1.0, 0.0, 0.0]           7       1\n3           310.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           4       1\n4           312.0      1  [0.0, 1.0, 0.0, 0.0, 0.0]           2       1",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Participant_ID</th>\n      <th>level</th>\n      <th>cat_level</th>\n      <th>PHQ8_Score</th>\n      <th>Gender</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>303.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>304.0</td>\n      <td>2</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>305.0</td>\n      <td>2</td>\n      <td>[0.0, 0.0, 1.0, 0.0, 0.0]</td>\n      <td>7</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>310.0</td>\n      <td>1</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n      <td>4</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>312.0</td>\n      <td>1</td>\n      <td>[0.0, 1.0, 0.0, 0.0, 0.0]</td>\n      <td>2</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "train = load_avec_dataset_file('train_split_Depression_AVEC2017 (1).csv','PHQ8_Score')\n",
    "dev = load_avec_dataset_file('dev_split_Depression_AVEC2017.csv','PHQ8_Score')\n",
    "test = load_avec_dataset_file('full_test_split.csv','PHQ8_Score')\n",
    "print(\"Size: train= {}, dev= {}, test={}\".format(len(train), len(dev), len(test)))\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Total size = 189\n"
    }
   ],
   "source": [
    "ds_total = pd.concat([dev,train,test])\n",
    "total_phq8 = len(ds_total)\n",
    "print(\"Total size = {}\".format(total_phq8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "File was created\n"
    }
   ],
   "source": [
    "ds_total.to_csv('ds_total.csv', sep='\\t')\n",
    "print(\"File was created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_phq_level(ds):\n",
    "    none_ds = ds[ds['level']==0]\n",
    "    mild_ds = ds[ds['level']==1]\n",
    "    moderate_ds = ds[ds['level']==2]\n",
    "    moderate_severe_ds = ds[ds['level']==3]\n",
    "    severe_ds = ds[ds['level']==4]\n",
    "    return (none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Quantity per none_ds: 26, mild_ds: 70, moderate_ds 47, moderate_severe_ds: 24, severe_ds 22\n"
    }
   ],
   "source": [
    "none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_total)\n",
    "print(\"Quantity per none_ds: {}, mild_ds: {}, moderate_ds {}, moderate_severe_ds: {}, severe_ds {}\".format(len(none_ds), len(mild_ds), len(moderate_ds), len(moderate_severe_ds), len(severe_ds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_none_ds = ds_total[ds_total['level']==0]\n",
    "b_mild_ds = ds_total[ds_total['level']==1].sample(26)\n",
    "b_moderate_ds = ds_total[ds_total['level']==2].sample(26)\n",
    "b_moderate_severe_ds = ds_total[ds_total['level']==3]\n",
    "b_severe_ds = ds_total[ds_total['level']==4]\n",
    "\n",
    "ds_total_b = pd.concat([b_none_ds, b_mild_ds, b_moderate_ds, b_moderate_severe_ds, b_severe_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_= ds_total_b.to_csv('ds_total_b.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_lp = pd.merge(ds_total, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
    "ds_lp.drop(ds_lp[ds_lp[\"t_answer\"].map(len) < 10].index, inplace = True)\n",
    "ds_lp_b = pd.merge(ds_total_b, phrases_lp,left_on='Participant_ID', right_on='personId')\n",
    "ds_lp_b.drop(ds_lp_b[ds_lp_b[\"t_answer\"].map(len) < 10].index, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_instances(ds, split_in = [70,14,16]):\n",
    "    ds_shuffled = ds.sample(frac=1)\n",
    "    none_ds, mild_ds, moderate_ds, moderate_severe_ds, severe_ds = split_by_phq_level(ds_shuffled)\n",
    "    eq_ds = dict()\n",
    "    prev_none = prev_mild = prev_moderate = prev_moderate_severe = prev_severe = 0\n",
    "    split = split_in\n",
    "    for p in split:\n",
    "        last_none = min(len(none_ds), prev_none + round(len(none_ds) * p/100))\n",
    "        last_mild = min(len(mild_ds), prev_mild + round(len(mild_ds) * p/100))\n",
    "        last_moderate = min(len(moderate_ds), prev_moderate + round(len(moderate_ds) * p/100))\n",
    "        last_moderate_severe = min(len(moderate_severe_ds), prev_moderate_severe + round(len(moderate_severe_ds) * p/100))\n",
    "        last_severe = min(len(severe_ds), prev_severe + round(len(severe_ds) * p/100))  \n",
    "        eq_ds['d'+str(p)] = pd.concat([none_ds[prev_none: last_none], mild_ds[prev_mild: last_mild], moderate_ds[prev_moderate: last_moderate], moderate_severe_ds[prev_moderate_severe: last_moderate_severe], severe_ds[prev_severe: last_severe]])\n",
    "        prev_none = last_none\n",
    "        prev_mild = last_mild\n",
    "        prev_moderate = last_moderate\n",
    "        prev_moderate_severe = last_moderate_severe\n",
    "        prev_severe = last_severe  \n",
    "    return (eq_ds['d70'], eq_ds['d14'], eq_ds['d16'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lp, dev_lp, test_lp = distribute_instances(ds_lp)\n",
    "train_lp_b, dev_lp_b, test_lp_b = distribute_instances(ds_lp_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_embedding_matrix(tokenizer):\n",
    "    vocab_size = len(tokenizer.word_index) # tokenizer.word_index is the list that consist of all the unique words\n",
    "    embedding_matrix = np.zeros((vocab_size+1, 100)) # creating an embedding matrix\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:        \n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "(7374, 100)"
     },
     "metadata": {},
     "execution_count": 44
    }
   ],
   "source": [
    "embedding_matrix_lp = fill_embedding_matrix(tokenizer)\n",
    "embedding_matrix_lp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "        Participant_ID  level                  cat_level  PHQ8_Score  Gender  \\\n95039            463.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n126186           411.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0   \n58187            370.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n124592           408.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       0   \n115800           361.0      0  [1.0, 0.0, 0.0, 0.0, 0.0]           0       1   \n\n        Unnamed: 0  personId  \\\n95039       121679       463   \n126186       82912       411   \n58187        51323       370   \n124592       80014       408   \n115800       40165       361   \n\n                                                   answer  \\\n95039   ['saleing', 'family', 'pretty', 'fun', 'going'...   \n126186  ['incredible', 'healing', 'thing', 'believe', ...   \n58187   ['even', 'though', 'could', 'get', 'full', 'ei...   \n124592  ['cannot', 'think', 'sorry', 'laughter', 'guil...   \n115800  ['like', 'kid', 'candy', 'store', 'get', 'play...   \n\n                                               t_answer  \n95039   [6855, 61, 23, 127, 40, 38, 1441, 1238, 616, 2]  \n126186      [1129, 3866, 9, 343, 2, 1, 2, 12, 553, 944]  \n58187    [78, 217, 37, 18, 705, 363, 213, 2257, 77, 50]  \n124592     [68, 12, 374, 8, 305, 12, 70, 28, 305, 1177]  \n115800     [5, 116, 2941, 857, 18, 288, 38, 4279, 1, 7]  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Participant_ID</th>\n      <th>level</th>\n      <th>cat_level</th>\n      <th>PHQ8_Score</th>\n      <th>Gender</th>\n      <th>Unnamed: 0</th>\n      <th>personId</th>\n      <th>answer</th>\n      <th>t_answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>95039</th>\n      <td>463.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>121679</td>\n      <td>463</td>\n      <td>['saleing', 'family', 'pretty', 'fun', 'going'...</td>\n      <td>[6855, 61, 23, 127, 40, 38, 1441, 1238, 616, 2]</td>\n    </tr>\n    <tr>\n      <th>126186</th>\n      <td>411.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>82912</td>\n      <td>411</td>\n      <td>['incredible', 'healing', 'thing', 'believe', ...</td>\n      <td>[1129, 3866, 9, 343, 2, 1, 2, 12, 553, 944]</td>\n    </tr>\n    <tr>\n      <th>58187</th>\n      <td>370.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>51323</td>\n      <td>370</td>\n      <td>['even', 'though', 'could', 'get', 'full', 'ei...</td>\n      <td>[78, 217, 37, 18, 705, 363, 213, 2257, 77, 50]</td>\n    </tr>\n    <tr>\n      <th>124592</th>\n      <td>408.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>0</td>\n      <td>80014</td>\n      <td>408</td>\n      <td>['cannot', 'think', 'sorry', 'laughter', 'guil...</td>\n      <td>[68, 12, 374, 8, 305, 12, 70, 28, 305, 1177]</td>\n    </tr>\n    <tr>\n      <th>115800</th>\n      <td>361.0</td>\n      <td>0</td>\n      <td>[1.0, 0.0, 0.0, 0.0, 0.0]</td>\n      <td>0</td>\n      <td>1</td>\n      <td>40165</td>\n      <td>361</td>\n      <td>['like', 'kid', 'candy', 'store', 'get', 'play...</td>\n      <td>[5, 116, 2941, 857, 18, 288, 38, 4279, 1, 7]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "train_lp['t_answer']\n",
    "train_lp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a =np.stack(train_lp['t_answer'])\n",
    "dev_a = np.stack(dev_lp['t_answer'])\n",
    "train_y = np.stack(train_lp['cat_level'], axis=0)\n",
    "dev_y = np.stack(dev_lp['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_a_b = np.stack(train_lp_b['t_answer'], axis=0)\n",
    "dev_a_b = np.stack(dev_lp_b['t_answer'], axis=0)\n",
    "train_y_b = np.stack(train_lp_b['cat_level'], axis=0)\n",
    "dev_y_b = np.stack(dev_lp_b['cat_level'], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_acc(history, title=\"Model Accuracy\"):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_loss(history, title=\"Model Loss\"):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Val'], loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_losses(history1, history2, name1=\"Red 1\", name2=\"Red 2\", title=\"Graph title\"):\n",
    "    plt.plot(history1.history['loss'], color=\"green\")\n",
    "    plt.plot(history1.history['val_loss'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['loss'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_loss'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2],\n",
    "               loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_compare_accs(history1, history2, name1=\"Red 1\",\n",
    "                      name2=\"Red 2\", title=\"Graph title\"):\n",
    "    \"\"\"Compara accuracies de dos entrenamientos con nombres name1 y name2\"\"\"\n",
    "    plt.plot(history1.history['acc'], color=\"green\")\n",
    "    plt.plot(history1.history['val_acc'], 'r--', color=\"green\")\n",
    "    plt.plot(history2.history['acc'], color=\"blue\")\n",
    "    plt.plot(history2.history['val_acc'], 'r--', color=\"blue\")\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train ' + name1, 'Val ' + name1, \n",
    "                'Train ' + name2, 'Val ' + name2], \n",
    "               loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "def plot_compare_multiple_metrics(history_array, names, colors, title=\"Graph title\", metric='acc'):  \n",
    "    legend = []\n",
    "    for i in range(0, len(history_array)):\n",
    "        plt.plot(history_array[i].history[metric], color=colors[i])\n",
    "        plt.plot(history_array[i].history['val_' + metric], 'r--', color=colors[i])\n",
    "        legend.append('Train ' + names[i])\n",
    "        legend.append('Val ' + names[i])\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')   \n",
    "    plt.axis\n",
    "    plt.legend(legend, \n",
    "               loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_1 (InputLayer)         [(None, 10)]              0         \n_________________________________________________________________\nembedding (Embedding)        (None, 10, 100)           737400    \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 10, 100)           400       \n_________________________________________________________________\nlstm (LSTM)                  (None, 10, 100)           80400     \n_________________________________________________________________\ndense (Dense)                (None, 10, 256)           25856     \n_________________________________________________________________\ndense_1 (Dense)              (None, 10, 256)           65792     \n_________________________________________________________________\nflatten (Flatten)            (None, 2560)              0         \n_________________________________________________________________\ndense_2 (Dense)              (None, 5)                 12805     \n=================================================================\nTotal params: 922,653\nTrainable params: 185,053\nNon-trainable params: 737,600\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "bt = BatchNormalization()(answer_emb1)\n",
    "lstm = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(bt)\n",
    "\n",
    "dense1 = Dense(units=256, activation=\"relu\")(lstm)\n",
    "dense2 = Dense(units=256, activation=\"relu\")(dense1)\n",
    "\n",
    "flatten = Flatten()(dense2)\n",
    "\n",
    "out = Dense(5,  activation='softmax')(flatten)\n",
    "\n",
    "model = Model(inputs=[answer_inp], outputs=[out])\n",
    "model.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 98795 samples, validate on 19760 samples\nEpoch 1/30\n98795/98795 [==============================] - 42s 430us/sample - loss: 1.4518 - accuracy: 0.3713 - val_loss: 1.3246 - val_accuracy: 0.4396\nEpoch 2/30\n98795/98795 [==============================] - 37s 378us/sample - loss: 1.2637 - accuracy: 0.4742 - val_loss: 1.0869 - val_accuracy: 0.5647\nEpoch 3/30\n98795/98795 [==============================] - 38s 388us/sample - loss: 1.0989 - accuracy: 0.5550 - val_loss: 0.9280 - val_accuracy: 0.6346\nEpoch 4/30\n98795/98795 [==============================] - 37s 377us/sample - loss: 0.9874 - accuracy: 0.6067 - val_loss: 0.8028 - val_accuracy: 0.6962\nEpoch 5/30\n98795/98795 [==============================] - 39s 397us/sample - loss: 0.9080 - accuracy: 0.6431 - val_loss: 0.7263 - val_accuracy: 0.7285\nEpoch 6/30\n98795/98795 [==============================] - 40s 406us/sample - loss: 0.8435 - accuracy: 0.6696 - val_loss: 0.6364 - val_accuracy: 0.7648\nEpoch 7/30\n98795/98795 [==============================] - 43s 440us/sample - loss: 0.7945 - accuracy: 0.6942 - val_loss: 0.5865 - val_accuracy: 0.7818\nEpoch 8/30\n98795/98795 [==============================] - 44s 445us/sample - loss: 0.7486 - accuracy: 0.7095 - val_loss: 0.5323 - val_accuracy: 0.8110\nEpoch 9/30\n98795/98795 [==============================] - 43s 436us/sample - loss: 0.7112 - accuracy: 0.7257 - val_loss: 0.4996 - val_accuracy: 0.8193\nEpoch 10/30\n98795/98795 [==============================] - 42s 430us/sample - loss: 0.6806 - accuracy: 0.7384 - val_loss: 0.4711 - val_accuracy: 0.8301\nEpoch 11/30\n98795/98795 [==============================] - 43s 439us/sample - loss: 0.6517 - accuracy: 0.7516 - val_loss: 0.4337 - val_accuracy: 0.8457\nEpoch 12/30\n98795/98795 [==============================] - 44s 442us/sample - loss: 0.6249 - accuracy: 0.7629 - val_loss: 0.3946 - val_accuracy: 0.8624\nEpoch 13/30\n98795/98795 [==============================] - 44s 447us/sample - loss: 0.5988 - accuracy: 0.7722 - val_loss: 0.3768 - val_accuracy: 0.8705\nEpoch 14/30\n98795/98795 [==============================] - 45s 454us/sample - loss: 0.5776 - accuracy: 0.7821 - val_loss: 0.3450 - val_accuracy: 0.8832\nEpoch 15/30\n98795/98795 [==============================] - 45s 453us/sample - loss: 0.5551 - accuracy: 0.7907 - val_loss: 0.3241 - val_accuracy: 0.8897\nEpoch 16/30\n98795/98795 [==============================] - 44s 450us/sample - loss: 0.5343 - accuracy: 0.7987 - val_loss: 0.3069 - val_accuracy: 0.8968\nEpoch 17/30\n98795/98795 [==============================] - 45s 452us/sample - loss: 0.5185 - accuracy: 0.8060 - val_loss: 0.2870 - val_accuracy: 0.9042\nEpoch 18/30\n98795/98795 [==============================] - 41s 420us/sample - loss: 0.5001 - accuracy: 0.8138 - val_loss: 0.2798 - val_accuracy: 0.9044\nEpoch 19/30\n98795/98795 [==============================] - 40s 409us/sample - loss: 0.4854 - accuracy: 0.8192 - val_loss: 0.2639 - val_accuracy: 0.9120\nEpoch 20/30\n98795/98795 [==============================] - 40s 403us/sample - loss: 0.4750 - accuracy: 0.8242 - val_loss: 0.2478 - val_accuracy: 0.9177\nEpoch 21/30\n98795/98795 [==============================] - 40s 403us/sample - loss: 0.4607 - accuracy: 0.8272 - val_loss: 0.2323 - val_accuracy: 0.9222\nEpoch 22/30\n98795/98795 [==============================] - 41s 412us/sample - loss: 0.4473 - accuracy: 0.8346 - val_loss: 0.2227 - val_accuracy: 0.9259\nEpoch 23/30\n98795/98795 [==============================] - 43s 434us/sample - loss: 0.4332 - accuracy: 0.8397 - val_loss: 0.2087 - val_accuracy: 0.9305\nEpoch 24/30\n98795/98795 [==============================] - 41s 415us/sample - loss: 0.4213 - accuracy: 0.8450 - val_loss: 0.2014 - val_accuracy: 0.9324\nEpoch 25/30\n98795/98795 [==============================] - 40s 405us/sample - loss: 0.4136 - accuracy: 0.8474 - val_loss: 0.1977 - val_accuracy: 0.9317\nEpoch 26/30\n98795/98795 [==============================] - 40s 409us/sample - loss: 0.4067 - accuracy: 0.8498 - val_loss: 0.1799 - val_accuracy: 0.9426\nEpoch 27/30\n98795/98795 [==============================] - 40s 408us/sample - loss: 0.3928 - accuracy: 0.8548 - val_loss: 0.1788 - val_accuracy: 0.9435\nEpoch 28/30\n98795/98795 [==============================] - 41s 417us/sample - loss: 0.3861 - accuracy: 0.8586 - val_loss: 0.1671 - val_accuracy: 0.9450\nEpoch 29/30\n98795/98795 [==============================] - 41s 413us/sample - loss: 0.3781 - accuracy: 0.8603 - val_loss: 0.1663 - val_accuracy: 0.9452\nEpoch 30/30\n98795/98795 [==============================] - 42s 429us/sample - loss: 0.3710 - accuracy: 0.8634 - val_loss: 0.1663 - val_accuracy: 0.9430\n"
    }
   ],
   "source": [
    "model_glove_lstm_hist = model.fit(train_a, train_y, validation_data=(dev_a, dev_y), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_glove_lstm.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 64316 samples, validate on 12864 samples\nEpoch 1/30\n64316/64316 [==============================] - 25s 384us/sample - loss: 0.3857 - accuracy: 0.8607 - val_loss: 0.0740 - val_accuracy: 0.9815\nEpoch 2/30\n64316/64316 [==============================] - 25s 382us/sample - loss: 0.3376 - accuracy: 0.8788 - val_loss: 0.0669 - val_accuracy: 0.9842\nEpoch 3/30\n64316/64316 [==============================] - 24s 375us/sample - loss: 0.3120 - accuracy: 0.8879 - val_loss: 0.0599 - val_accuracy: 0.9845\nEpoch 4/30\n64316/64316 [==============================] - 24s 373us/sample - loss: 0.2871 - accuracy: 0.8972 - val_loss: 0.0559 - val_accuracy: 0.9854\nEpoch 5/30\n64316/64316 [==============================] - 24s 378us/sample - loss: 0.2786 - accuracy: 0.9013 - val_loss: 0.0551 - val_accuracy: 0.9854\nEpoch 6/30\n64316/64316 [==============================] - 25s 382us/sample - loss: 0.2646 - accuracy: 0.9046 - val_loss: 0.0542 - val_accuracy: 0.9852\nEpoch 7/30\n64316/64316 [==============================] - 25s 393us/sample - loss: 0.2565 - accuracy: 0.9080 - val_loss: 0.0499 - val_accuracy: 0.9864\nEpoch 8/30\n64316/64316 [==============================] - 26s 410us/sample - loss: 0.2457 - accuracy: 0.9132 - val_loss: 0.0482 - val_accuracy: 0.9855\nEpoch 9/30\n64316/64316 [==============================] - 27s 426us/sample - loss: 0.2385 - accuracy: 0.9138 - val_loss: 0.0496 - val_accuracy: 0.9852\nEpoch 10/30\n64316/64316 [==============================] - 30s 461us/sample - loss: 0.2344 - accuracy: 0.9162 - val_loss: 0.0469 - val_accuracy: 0.9873\nEpoch 11/30\n64316/64316 [==============================] - 33s 513us/sample - loss: 0.2197 - accuracy: 0.9232 - val_loss: 0.0473 - val_accuracy: 0.9861\nEpoch 12/30\n64316/64316 [==============================] - 33s 512us/sample - loss: 0.2254 - accuracy: 0.9194 - val_loss: 0.0451 - val_accuracy: 0.9864\nEpoch 13/30\n64316/64316 [==============================] - 31s 487us/sample - loss: 0.2134 - accuracy: 0.9245 - val_loss: 0.0421 - val_accuracy: 0.9869\nEpoch 14/30\n64316/64316 [==============================] - 33s 508us/sample - loss: 0.2127 - accuracy: 0.9247 - val_loss: 0.0427 - val_accuracy: 0.9862\nEpoch 15/30\n64316/64316 [==============================] - 33s 508us/sample - loss: 0.2052 - accuracy: 0.9274 - val_loss: 0.0414 - val_accuracy: 0.9867\nEpoch 16/30\n64316/64316 [==============================] - 33s 514us/sample - loss: 0.1968 - accuracy: 0.9301 - val_loss: 0.0438 - val_accuracy: 0.9877\nEpoch 17/30\n64316/64316 [==============================] - 29s 444us/sample - loss: 0.1973 - accuracy: 0.9300 - val_loss: 0.0406 - val_accuracy: 0.9875\nEpoch 18/30\n64316/64316 [==============================] - 25s 388us/sample - loss: 0.1934 - accuracy: 0.9316 - val_loss: 0.0415 - val_accuracy: 0.9876\nEpoch 19/30\n64316/64316 [==============================] - 26s 411us/sample - loss: 0.1913 - accuracy: 0.9319 - val_loss: 0.0377 - val_accuracy: 0.9877\nEpoch 20/30\n64316/64316 [==============================] - 26s 411us/sample - loss: 0.1863 - accuracy: 0.9339 - val_loss: 0.0357 - val_accuracy: 0.9900\nEpoch 21/30\n64316/64316 [==============================] - 27s 417us/sample - loss: 0.1831 - accuracy: 0.9351 - val_loss: 0.0367 - val_accuracy: 0.9900\nEpoch 22/30\n64316/64316 [==============================] - 27s 417us/sample - loss: 0.1844 - accuracy: 0.9349 - val_loss: 0.0389 - val_accuracy: 0.9885\nEpoch 23/30\n64316/64316 [==============================] - 26s 409us/sample - loss: 0.1832 - accuracy: 0.9356 - val_loss: 0.0361 - val_accuracy: 0.9886\n"
    }
   ],
   "source": [
    "model_glove_lstm_hist_b = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_glove_lstm_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 10)]              0         \n_________________________________________________________________\nembedding_1 (Embedding)      (None, 10, 100)           737400    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 10, 100)           80400     \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndropout (Dropout)            (None, 100)               0         \n_________________________________________________________________\nbatch_normalization_1 (Batch (None, 100)               400       \n_________________________________________________________________\ndense_3 (Dense)              (None, 256)               25856     \n_________________________________________________________________\ndense_4 (Dense)              (None, 5)                 1285      \n=================================================================\nTotal params: 925,741\nTrainable params: 188,141\nNon-trainable params: 737,600\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "\n",
    "lstm1 = LSTM(embedding_size_glove, return_sequences=True, dropout=0.2, recurrent_dropout=0.2)(answer_emb1)\n",
    "lstm2 = LSTM(embedding_size_glove, dropout=0.2, recurrent_dropout=0.2)(lstm1)\n",
    "\n",
    "X = Dropout(0.2)(lstm2)\n",
    "bt = BatchNormalization()(X)\n",
    "dense1 = Dense(units=256, activation=\"relu\")(bt)\n",
    "\n",
    "out = Dense(5,  activation='softmax')(dense1)\n",
    "\n",
    "model_2lstm = Model(inputs=[answer_inp], outputs=[out])\n",
    "model_2lstm.compile(optimizer=\"adam\", loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model_2lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 64316 samples, validate on 12864 samples\nEpoch 1/30\n64316/64316 [==============================] - 46s 720us/sample - loss: 1.6160 - accuracy: 0.2439 - val_loss: 1.5850 - val_accuracy: 0.2614\nEpoch 2/30\n64316/64316 [==============================] - 37s 569us/sample - loss: 1.5728 - accuracy: 0.2797 - val_loss: 1.5526 - val_accuracy: 0.2996\nEpoch 3/30\n64316/64316 [==============================] - 36s 566us/sample - loss: 1.5345 - accuracy: 0.3125 - val_loss: 1.4871 - val_accuracy: 0.3500\nEpoch 4/30\n64316/64316 [==============================] - 39s 601us/sample - loss: 1.4722 - accuracy: 0.3624 - val_loss: 1.3962 - val_accuracy: 0.4082\nEpoch 5/30\n64316/64316 [==============================] - 42s 651us/sample - loss: 1.3924 - accuracy: 0.4111 - val_loss: 1.2893 - val_accuracy: 0.4737\nEpoch 6/30\n64316/64316 [==============================] - 51s 799us/sample - loss: 1.3083 - accuracy: 0.4600 - val_loss: 1.1948 - val_accuracy: 0.5283\nEpoch 7/30\n64316/64316 [==============================] - 43s 671us/sample - loss: 1.2356 - accuracy: 0.4995 - val_loss: 1.1230 - val_accuracy: 0.5570\nEpoch 8/30\n64316/64316 [==============================] - 39s 602us/sample - loss: 1.1649 - accuracy: 0.5338 - val_loss: 1.0373 - val_accuracy: 0.5962\nEpoch 9/30\n64316/64316 [==============================] - 40s 618us/sample - loss: 1.1095 - accuracy: 0.5594 - val_loss: 0.9695 - val_accuracy: 0.6342\nEpoch 10/30\n64316/64316 [==============================] - 39s 604us/sample - loss: 1.0589 - accuracy: 0.5819 - val_loss: 0.9041 - val_accuracy: 0.6623\nEpoch 11/30\n64316/64316 [==============================] - 39s 607us/sample - loss: 1.0135 - accuracy: 0.6025 - val_loss: 0.8629 - val_accuracy: 0.6779\nEpoch 12/30\n64316/64316 [==============================] - 39s 607us/sample - loss: 0.9810 - accuracy: 0.6180 - val_loss: 0.8039 - val_accuracy: 0.7020\nEpoch 13/30\n64316/64316 [==============================] - 39s 603us/sample - loss: 0.9440 - accuracy: 0.6347 - val_loss: 0.7571 - val_accuracy: 0.7258\nEpoch 14/30\n64316/64316 [==============================] - 38s 590us/sample - loss: 0.9136 - accuracy: 0.6479 - val_loss: 0.7251 - val_accuracy: 0.7381\nEpoch 15/30\n64316/64316 [==============================] - 38s 588us/sample - loss: 0.8952 - accuracy: 0.6531 - val_loss: 0.7220 - val_accuracy: 0.7394\nEpoch 16/30\n64316/64316 [==============================] - 39s 600us/sample - loss: 0.8677 - accuracy: 0.6684 - val_loss: 0.6731 - val_accuracy: 0.7622\nEpoch 17/30\n64316/64316 [==============================] - 38s 597us/sample - loss: 0.8499 - accuracy: 0.6745 - val_loss: 0.6489 - val_accuracy: 0.7662\nEpoch 18/30\n64316/64316 [==============================] - 38s 593us/sample - loss: 0.8316 - accuracy: 0.6834 - val_loss: 0.6216 - val_accuracy: 0.7819\nEpoch 19/30\n64316/64316 [==============================] - 38s 591us/sample - loss: 0.8187 - accuracy: 0.6876 - val_loss: 0.5995 - val_accuracy: 0.7883\nEpoch 20/30\n64316/64316 [==============================] - 39s 601us/sample - loss: 0.7972 - accuracy: 0.6981 - val_loss: 0.5764 - val_accuracy: 0.7988\nEpoch 21/30\n64316/64316 [==============================] - 38s 592us/sample - loss: 0.7890 - accuracy: 0.7004 - val_loss: 0.5561 - val_accuracy: 0.8085\nEpoch 22/30\n64316/64316 [==============================] - 38s 593us/sample - loss: 0.7725 - accuracy: 0.7078 - val_loss: 0.5399 - val_accuracy: 0.8124\nEpoch 23/30\n64316/64316 [==============================] - 38s 596us/sample - loss: 0.7544 - accuracy: 0.7158 - val_loss: 0.5256 - val_accuracy: 0.8195\nEpoch 24/30\n64316/64316 [==============================] - 38s 590us/sample - loss: 0.7506 - accuracy: 0.7164 - val_loss: 0.5178 - val_accuracy: 0.8260\nEpoch 25/30\n64316/64316 [==============================] - 48s 746us/sample - loss: 0.7395 - accuracy: 0.7215 - val_loss: 0.5024 - val_accuracy: 0.8343\nEpoch 26/30\n64316/64316 [==============================] - 48s 752us/sample - loss: 0.7238 - accuracy: 0.7275 - val_loss: 0.4816 - val_accuracy: 0.8318\nEpoch 27/30\n64316/64316 [==============================] - 47s 729us/sample - loss: 0.7165 - accuracy: 0.7280 - val_loss: 0.4697 - val_accuracy: 0.8416\nEpoch 28/30\n64316/64316 [==============================] - 36s 565us/sample - loss: 0.7115 - accuracy: 0.7332 - val_loss: 0.4631 - val_accuracy: 0.8427\nEpoch 29/30\n64316/64316 [==============================] - 38s 590us/sample - loss: 0.6968 - accuracy: 0.7378 - val_loss: 0.4420 - val_accuracy: 0.8547\nEpoch 30/30\n64316/64316 [==============================] - 38s 587us/sample - loss: 0.6938 - accuracy: 0.7409 - val_loss: 0.4276 - val_accuracy: 0.8568\n"
    }
   ],
   "source": [
    "model_glove_2lstm_b_hist = model_2lstm.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2lstm.save('model_2lstm_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_2\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_3 (InputLayer)         [(None, 10)]              0         \n_________________________________________________________________\nembedding_2 (Embedding)      (None, 10, 100)           737400    \n_________________________________________________________________\nbidirectional (Bidirectional (None, 10, 200)           160800    \n_________________________________________________________________\ntime_distributed (TimeDistri (None, 10, 100)           20100     \n_________________________________________________________________\nflatten_1 (Flatten)          (None, 1000)              0         \n_________________________________________________________________\ndense_6 (Dense)              (None, 256)               256256    \n_________________________________________________________________\ndense_7 (Dense)              (None, 5)                 1285      \n=================================================================\nTotal params: 1,175,841\nTrainable params: 438,441\nNon-trainable params: 737,400\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "# main model\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "\n",
    "bi_lstm =  Bidirectional (LSTM (embedding_size_glove,return_sequences=True,dropout=0.50),merge_mode='concat')(answer_emb1)\n",
    "model_bi1 = TimeDistributed(Dense(embedding_size_glove,activation='relu'))(bi_lstm) #TimeDistributed method is used to apply a Dense layer to each of the time-steps independently. We used Dropout and l2_reg regularizers to reduce overfitting.\n",
    "model_bi2 = Flatten()(model_bi1)\n",
    "model_bi3 = Dense(256,activation='relu')(model_bi2)\n",
    "output = Dense(5,activation='softmax')(model_bi3)\n",
    "model_bi = Model(answer_inp,output)\n",
    "model_bi.compile(loss='categorical_crossentropy',optimizer='adam', metrics=['accuracy'])\n",
    "model_bi.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 64316 samples, validate on 12864 samples\nEpoch 1/30\n64316/64316 [==============================] - 33s 508us/sample - loss: 1.5807 - accuracy: 0.2648 - val_loss: 1.5552 - val_accuracy: 0.2844\nEpoch 2/30\n64316/64316 [==============================] - 26s 397us/sample - loss: 1.5456 - accuracy: 0.2996 - val_loss: 1.5061 - val_accuracy: 0.3270\nEpoch 3/30\n64316/64316 [==============================] - 26s 399us/sample - loss: 1.4966 - accuracy: 0.3356 - val_loss: 1.4572 - val_accuracy: 0.3714\nEpoch 4/30\n64316/64316 [==============================] - 26s 402us/sample - loss: 1.4332 - accuracy: 0.3801 - val_loss: 1.3715 - val_accuracy: 0.4317\nEpoch 5/30\n64316/64316 [==============================] - 26s 401us/sample - loss: 1.3689 - accuracy: 0.4199 - val_loss: 1.2754 - val_accuracy: 0.4777\nEpoch 6/30\n64316/64316 [==============================] - 26s 401us/sample - loss: 1.3079 - accuracy: 0.4546 - val_loss: 1.1951 - val_accuracy: 0.5257\nEpoch 7/30\n64316/64316 [==============================] - 26s 397us/sample - loss: 1.2478 - accuracy: 0.4859 - val_loss: 1.1385 - val_accuracy: 0.5548\nEpoch 8/30\n64316/64316 [==============================] - 26s 408us/sample - loss: 1.1968 - accuracy: 0.5119 - val_loss: 1.0572 - val_accuracy: 0.5925\nEpoch 9/30\n64316/64316 [==============================] - 26s 404us/sample - loss: 1.1567 - accuracy: 0.5306 - val_loss: 1.0014 - val_accuracy: 0.6179\nEpoch 10/30\n64316/64316 [==============================] - 27s 421us/sample - loss: 1.1184 - accuracy: 0.5480 - val_loss: 0.9522 - val_accuracy: 0.6392\nEpoch 11/30\n64316/64316 [==============================] - 27s 418us/sample - loss: 1.0781 - accuracy: 0.5677 - val_loss: 0.9167 - val_accuracy: 0.6549\nEpoch 12/30\n64316/64316 [==============================] - 27s 413us/sample - loss: 1.0429 - accuracy: 0.5822 - val_loss: 0.8785 - val_accuracy: 0.6716\nEpoch 13/30\n64316/64316 [==============================] - 27s 426us/sample - loss: 1.0149 - accuracy: 0.5968 - val_loss: 0.8537 - val_accuracy: 0.6747\nEpoch 14/30\n64316/64316 [==============================] - 29s 445us/sample - loss: 0.9859 - accuracy: 0.6091 - val_loss: 0.8153 - val_accuracy: 0.6954\nEpoch 15/30\n64316/64316 [==============================] - 29s 448us/sample - loss: 0.9543 - accuracy: 0.6252 - val_loss: 0.7877 - val_accuracy: 0.7042\nEpoch 16/30\n64316/64316 [==============================] - 29s 448us/sample - loss: 0.9272 - accuracy: 0.6381 - val_loss: 0.7830 - val_accuracy: 0.7088\nEpoch 17/30\n64316/64316 [==============================] - 28s 443us/sample - loss: 0.9079 - accuracy: 0.6454 - val_loss: 0.7615 - val_accuracy: 0.7141\nEpoch 18/30\n64316/64316 [==============================] - 28s 436us/sample - loss: 0.8823 - accuracy: 0.6569 - val_loss: 0.7507 - val_accuracy: 0.7227\nEpoch 19/30\n64316/64316 [==============================] - 28s 435us/sample - loss: 0.8648 - accuracy: 0.6658 - val_loss: 0.7442 - val_accuracy: 0.7213\nEpoch 20/30\n64316/64316 [==============================] - 28s 432us/sample - loss: 0.8386 - accuracy: 0.6756 - val_loss: 0.7285 - val_accuracy: 0.7252\nEpoch 21/30\n64316/64316 [==============================] - 28s 434us/sample - loss: 0.8169 - accuracy: 0.6851 - val_loss: 0.7173 - val_accuracy: 0.7290\nEpoch 22/30\n64316/64316 [==============================] - 28s 432us/sample - loss: 0.8031 - accuracy: 0.6909 - val_loss: 0.7111 - val_accuracy: 0.7339\nEpoch 23/30\n64316/64316 [==============================] - 28s 443us/sample - loss: 0.7844 - accuracy: 0.6986 - val_loss: 0.6898 - val_accuracy: 0.7428\nEpoch 24/30\n64316/64316 [==============================] - 28s 439us/sample - loss: 0.7637 - accuracy: 0.7095 - val_loss: 0.7022 - val_accuracy: 0.7386\nEpoch 25/30\n64316/64316 [==============================] - 28s 433us/sample - loss: 0.7488 - accuracy: 0.7140 - val_loss: 0.6885 - val_accuracy: 0.7407\nEpoch 26/30\n64316/64316 [==============================] - 28s 431us/sample - loss: 0.7366 - accuracy: 0.7187 - val_loss: 0.7025 - val_accuracy: 0.7377\nEpoch 27/30\n64316/64316 [==============================] - 30s 472us/sample - loss: 0.7254 - accuracy: 0.7231 - val_loss: 0.7027 - val_accuracy: 0.7382\nEpoch 28/30\n64316/64316 [==============================] - 28s 440us/sample - loss: 0.7106 - accuracy: 0.7296 - val_loss: 0.6996 - val_accuracy: 0.7395\n"
    }
   ],
   "source": [
    "model_glove_bilstm = model_bi.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bi.save('model_bilstm_a_b.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from tensorflow.keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_3\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_4 (InputLayer)            [(None, 10)]         0                                            \n__________________________________________________________________________________________________\nembedding_3 (Embedding)         (None, 10, 100)      737400      input_4[0][0]                    \n__________________________________________________________________________________________________\nspatial_dropout1d (SpatialDropo (None, 10, 100)      0           embedding_3[0][0]                \n__________________________________________________________________________________________________\nbidirectional_1 (Bidirectional) (None, 10, 200)      121200      spatial_dropout1d[0][0]          \n__________________________________________________________________________________________________\nglobal_average_pooling1d (Globa (None, 200)          0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nglobal_max_pooling1d (GlobalMax (None, 200)          0           bidirectional_1[0][0]            \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 400)          0           global_average_pooling1d[0][0]   \n                                                                 global_max_pooling1d[0][0]       \n__________________________________________________________________________________________________\ndense_8 (Dense)                 (None, 5)            2005        concatenate[0][0]                \n==================================================================================================\nTotal params: 860,605\nTrainable params: 123,205\nNon-trainable params: 737,400\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "# main model\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "\n",
    "x = SpatialDropout1D(0.2)(answer_emb1)\n",
    "x = Bidirectional(GRU(embedding_size_glove, return_sequences=True))(x)\n",
    "avg_pool = GlobalAveragePooling1D()(x)\n",
    "max_pool = GlobalMaxPooling1D()(x)\n",
    "conc = concatenate([avg_pool, max_pool])\n",
    "outp = Dense(5, activation=\"softmax\")(conc)\n",
    "    \n",
    "model_gru = Model(inputs=answer_inp, outputs=outp)\n",
    "model_gru.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model_gru.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "epochs = 30\n",
    "RocAuc = RocAucEvaluation(validation_data=(dev_a,dev_y), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 98795 samples, validate on 19760 samples\nEpoch 1/30\n\n ROC-AUC - epoch: 1 - score: 0.672460 \n\n98795/98795 - 37s - loss: 1.4750 - accuracy: 0.3645 - val_loss: 1.4270 - val_accuracy: 0.3881\nEpoch 2/30\n\n ROC-AUC - epoch: 2 - score: 0.757883 \n\n98795/98795 - 35s - loss: 1.3814 - accuracy: 0.4194 - val_loss: 1.3081 - val_accuracy: 0.4523\nEpoch 3/30\n\n ROC-AUC - epoch: 3 - score: 0.818856 \n\n98795/98795 - 33s - loss: 1.2566 - accuracy: 0.4887 - val_loss: 1.1584 - val_accuracy: 0.5530\nEpoch 4/30\n\n ROC-AUC - epoch: 4 - score: 0.859144 \n\n98795/98795 - 33s - loss: 1.1424 - accuracy: 0.5469 - val_loss: 1.0397 - val_accuracy: 0.6025\nEpoch 5/30\n\n ROC-AUC - epoch: 5 - score: 0.884402 \n\n98795/98795 - 33s - loss: 1.0448 - accuracy: 0.5936 - val_loss: 0.9622 - val_accuracy: 0.6352\nEpoch 6/30\n\n ROC-AUC - epoch: 6 - score: 0.909178 \n\n98795/98795 - 33s - loss: 0.9663 - accuracy: 0.6264 - val_loss: 0.8410 - val_accuracy: 0.6883\nEpoch 7/30\n\n ROC-AUC - epoch: 7 - score: 0.922521 \n\n98795/98795 - 34s - loss: 0.9089 - accuracy: 0.6500 - val_loss: 0.7850 - val_accuracy: 0.7100\nEpoch 8/30\n\n ROC-AUC - epoch: 8 - score: 0.932917 \n\n98795/98795 - 34s - loss: 0.8588 - accuracy: 0.6719 - val_loss: 0.7306 - val_accuracy: 0.7339\nEpoch 9/30\n\n ROC-AUC - epoch: 9 - score: 0.941739 \n\n98795/98795 - 32s - loss: 0.8158 - accuracy: 0.6899 - val_loss: 0.6727 - val_accuracy: 0.7584\nEpoch 10/30\n\n ROC-AUC - epoch: 10 - score: 0.947680 \n\n98795/98795 - 32s - loss: 0.7802 - accuracy: 0.7036 - val_loss: 0.6478 - val_accuracy: 0.7698\nEpoch 11/30\n\n ROC-AUC - epoch: 11 - score: 0.955312 \n\n98795/98795 - 32s - loss: 0.7570 - accuracy: 0.7122 - val_loss: 0.5913 - val_accuracy: 0.7906\nEpoch 12/30\n\n ROC-AUC - epoch: 12 - score: 0.958353 \n\n98795/98795 - 32s - loss: 0.7272 - accuracy: 0.7237 - val_loss: 0.5808 - val_accuracy: 0.7920\nEpoch 13/30\n\n ROC-AUC - epoch: 13 - score: 0.961011 \n\n98795/98795 - 35s - loss: 0.7081 - accuracy: 0.7327 - val_loss: 0.5748 - val_accuracy: 0.7910\nEpoch 14/30\n\n ROC-AUC - epoch: 14 - score: 0.963913 \n\n98795/98795 - 33s - loss: 0.6867 - accuracy: 0.7416 - val_loss: 0.5383 - val_accuracy: 0.8079\nEpoch 15/30\n\n ROC-AUC - epoch: 15 - score: 0.966621 \n\n98795/98795 - 32s - loss: 0.6724 - accuracy: 0.7464 - val_loss: 0.5170 - val_accuracy: 0.8140\nEpoch 16/30\n\n ROC-AUC - epoch: 16 - score: 0.970143 \n\n98795/98795 - 32s - loss: 0.6554 - accuracy: 0.7529 - val_loss: 0.4814 - val_accuracy: 0.8319\nEpoch 17/30\n\n ROC-AUC - epoch: 17 - score: 0.970821 \n\n98795/98795 - 32s - loss: 0.6443 - accuracy: 0.7581 - val_loss: 0.4868 - val_accuracy: 0.8243\nEpoch 18/30\n\n ROC-AUC - epoch: 18 - score: 0.973584 \n\n98795/98795 - 32s - loss: 0.6283 - accuracy: 0.7630 - val_loss: 0.4515 - val_accuracy: 0.8435\nEpoch 19/30\n\n ROC-AUC - epoch: 19 - score: 0.974485 \n\n98795/98795 - 32s - loss: 0.6185 - accuracy: 0.7673 - val_loss: 0.4465 - val_accuracy: 0.8423\nEpoch 20/30\n\n ROC-AUC - epoch: 20 - score: 0.975377 \n\n98795/98795 - 32s - loss: 0.6141 - accuracy: 0.7697 - val_loss: 0.4528 - val_accuracy: 0.8400\nEpoch 21/30\n\n ROC-AUC - epoch: 21 - score: 0.977051 \n\n98795/98795 - 32s - loss: 0.6042 - accuracy: 0.7727 - val_loss: 0.4348 - val_accuracy: 0.8476\nEpoch 22/30\n\n ROC-AUC - epoch: 22 - score: 0.977566 \n\n98795/98795 - 31s - loss: 0.5944 - accuracy: 0.7769 - val_loss: 0.4234 - val_accuracy: 0.8526\nEpoch 23/30\n\n ROC-AUC - epoch: 23 - score: 0.978248 \n\n98795/98795 - 32s - loss: 0.5860 - accuracy: 0.7804 - val_loss: 0.4183 - val_accuracy: 0.8499\nEpoch 24/30\n\n ROC-AUC - epoch: 24 - score: 0.979987 \n\n98795/98795 - 32s - loss: 0.5782 - accuracy: 0.7835 - val_loss: 0.4036 - val_accuracy: 0.8600\nEpoch 25/30\n\n ROC-AUC - epoch: 25 - score: 0.978729 \n\n98795/98795 - 32s - loss: 0.5738 - accuracy: 0.7859 - val_loss: 0.4175 - val_accuracy: 0.8538\nEpoch 26/30\n\n ROC-AUC - epoch: 26 - score: 0.980717 \n\n98795/98795 - 34s - loss: 0.5609 - accuracy: 0.7893 - val_loss: 0.3965 - val_accuracy: 0.8590\nEpoch 27/30\n\n ROC-AUC - epoch: 27 - score: 0.982068 \n\n98795/98795 - 32s - loss: 0.5548 - accuracy: 0.7913 - val_loss: 0.3727 - val_accuracy: 0.8715\nEpoch 28/30\n\n ROC-AUC - epoch: 28 - score: 0.982061 \n\n98795/98795 - 32s - loss: 0.5516 - accuracy: 0.7951 - val_loss: 0.3747 - val_accuracy: 0.8691\nEpoch 29/30\n\n ROC-AUC - epoch: 29 - score: 0.981785 \n\n98795/98795 - 32s - loss: 0.5481 - accuracy: 0.7941 - val_loss: 0.3909 - val_accuracy: 0.8632\nEpoch 30/30\n\n ROC-AUC - epoch: 30 - score: 0.984057 \n\n98795/98795 - 32s - loss: 0.5467 - accuracy: 0.7955 - val_loss: 0.3507 - val_accuracy: 0.8789\n"
    }
   ],
   "source": [
    "hist = model_gru.fit(train_a, train_y, batch_size=batch_size, epochs=epochs, validation_data=(dev_a, dev_y),\n",
    "                 callbacks=[RocAuc], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_gru.save('model_gru.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"model_4\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_5 (InputLayer)            [(None, 10)]         0                                            \n__________________________________________________________________________________________________\nembedding_4 (Embedding)         (None, 10, 100)      737400      input_5[0][0]                    \n__________________________________________________________________________________________________\nreshape (Reshape)               (None, 10, 100, 1)   0           embedding_4[0][0]                \n__________________________________________________________________________________________________\nconv2d (Conv2D)                 (None, 10, 1, 36)    3636        reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_1 (Conv2D)               (None, 9, 1, 36)     7236        reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_2 (Conv2D)               (None, 8, 1, 36)     10836       reshape[0][0]                    \n__________________________________________________________________________________________________\nconv2d_3 (Conv2D)               (None, 6, 1, 36)     18036       reshape[0][0]                    \n__________________________________________________________________________________________________\nmax_pooling2d (MaxPooling2D)    (None, 1, 1, 36)     0           conv2d[0][0]                     \n__________________________________________________________________________________________________\nmax_pooling2d_1 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_1[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_2 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_2[0][0]                   \n__________________________________________________________________________________________________\nmax_pooling2d_3 (MaxPooling2D)  (None, 1, 1, 36)     0           conv2d_3[0][0]                   \n__________________________________________________________________________________________________\nconcatenate_1 (Concatenate)     (None, 4, 1, 36)     0           max_pooling2d[0][0]              \n                                                                 max_pooling2d_1[0][0]            \n                                                                 max_pooling2d_2[0][0]            \n                                                                 max_pooling2d_3[0][0]            \n__________________________________________________________________________________________________\nflatten_2 (Flatten)             (None, 144)          0           concatenate_1[0][0]              \n__________________________________________________________________________________________________\ndropout_1 (Dropout)             (None, 144)          0           flatten_2[0][0]                  \n__________________________________________________________________________________________________\ndense_9 (Dense)                 (None, 5)            725         dropout_1[0][0]                  \n==================================================================================================\nTotal params: 777,869\nTrainable params: 40,469\nNon-trainable params: 737,400\n__________________________________________________________________________________________________\n"
    }
   ],
   "source": [
    "filter_sizes = [1,2,3,5]\n",
    "num_filters = 36\n",
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)\n",
    "x = Reshape((windows_size, embedding_size_glove, 1))(answer_emb1)\n",
    "maxpool_pool = []\n",
    "for i in range(len(filter_sizes)):\n",
    "  conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embedding_size_glove),kernel_initializer='he_normal', activation='elu')(x)\n",
    "  maxpool_pool.append(MaxPool2D(pool_size=(windows_size - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "z = Concatenate(axis=1)(maxpool_pool)   \n",
    "z = Flatten()(z)\n",
    "z = Dropout(0.1)(z)\n",
    "\n",
    "outp = Dense(5, activation=\"softmax\")(z)\n",
    "\n",
    "model = Model(inputs=answer_inp, outputs=outp)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 98795 samples, validate on 19760 samples\nEpoch 1/30\n98795/98795 - 8s - loss: 1.4535 - accuracy: 0.3803 - val_loss: 1.3552 - val_accuracy: 0.4138\nEpoch 2/30\n98795/98795 - 8s - loss: 1.2482 - accuracy: 0.4899 - val_loss: 1.2066 - val_accuracy: 0.5063\nEpoch 3/30\n98795/98795 - 9s - loss: 1.0916 - accuracy: 0.5653 - val_loss: 1.0926 - val_accuracy: 0.5616\nEpoch 4/30\n98795/98795 - 9s - loss: 0.9770 - accuracy: 0.6166 - val_loss: 1.0281 - val_accuracy: 0.5894\nEpoch 5/30\n98795/98795 - 9s - loss: 0.8971 - accuracy: 0.6503 - val_loss: 0.9745 - val_accuracy: 0.6221\nEpoch 6/30\n98795/98795 - 9s - loss: 0.8382 - accuracy: 0.6755 - val_loss: 0.8786 - val_accuracy: 0.6598\nEpoch 7/30\n98795/98795 - 9s - loss: 0.7913 - accuracy: 0.6955 - val_loss: 0.8387 - val_accuracy: 0.6808\nEpoch 8/30\n98795/98795 - 9s - loss: 0.7545 - accuracy: 0.7111 - val_loss: 0.8131 - val_accuracy: 0.6906\nEpoch 9/30\n98795/98795 - 9s - loss: 0.7238 - accuracy: 0.7227 - val_loss: 0.8084 - val_accuracy: 0.6938\nEpoch 10/30\n98795/98795 - 9s - loss: 0.6981 - accuracy: 0.7325 - val_loss: 0.7770 - val_accuracy: 0.7036\nEpoch 11/30\n98795/98795 - 9s - loss: 0.6769 - accuracy: 0.7423 - val_loss: 0.7405 - val_accuracy: 0.7199\nEpoch 12/30\n98795/98795 - 9s - loss: 0.6563 - accuracy: 0.7482 - val_loss: 0.7537 - val_accuracy: 0.7135\nEpoch 13/30\n98795/98795 - 10s - loss: 0.6379 - accuracy: 0.7563 - val_loss: 0.6989 - val_accuracy: 0.7355\nEpoch 14/30\n98795/98795 - 10s - loss: 0.6252 - accuracy: 0.7618 - val_loss: 0.6820 - val_accuracy: 0.7433\nEpoch 15/30\n98795/98795 - 10s - loss: 0.6048 - accuracy: 0.7703 - val_loss: 0.7088 - val_accuracy: 0.7351\nEpoch 16/30\n98795/98795 - 10s - loss: 0.5960 - accuracy: 0.7729 - val_loss: 0.6610 - val_accuracy: 0.7517\nEpoch 17/30\n98795/98795 - 10s - loss: 0.5810 - accuracy: 0.7793 - val_loss: 0.6779 - val_accuracy: 0.7436\nEpoch 18/30\n98795/98795 - 10s - loss: 0.5732 - accuracy: 0.7829 - val_loss: 0.6591 - val_accuracy: 0.7529\nEpoch 19/30\n98795/98795 - 10s - loss: 0.5598 - accuracy: 0.7888 - val_loss: 0.6441 - val_accuracy: 0.7614\nEpoch 20/30\n98795/98795 - 10s - loss: 0.5499 - accuracy: 0.7898 - val_loss: 0.6207 - val_accuracy: 0.7713\nEpoch 21/30\n98795/98795 - 10s - loss: 0.5421 - accuracy: 0.7949 - val_loss: 0.6252 - val_accuracy: 0.7648\nEpoch 22/30\n98795/98795 - 10s - loss: 0.5350 - accuracy: 0.7965 - val_loss: 0.6062 - val_accuracy: 0.7739\nEpoch 23/30\n98795/98795 - 10s - loss: 0.5282 - accuracy: 0.7982 - val_loss: 0.6028 - val_accuracy: 0.7723\nEpoch 24/30\n98795/98795 - 10s - loss: 0.5154 - accuracy: 0.8054 - val_loss: 0.5785 - val_accuracy: 0.7849\nEpoch 25/30\n98795/98795 - 10s - loss: 0.5106 - accuracy: 0.8060 - val_loss: 0.6145 - val_accuracy: 0.7707\nEpoch 26/30\n98795/98795 - 10s - loss: 0.5045 - accuracy: 0.8085 - val_loss: 0.5997 - val_accuracy: 0.7757\nEpoch 27/30\n98795/98795 - 10s - loss: 0.4984 - accuracy: 0.8123 - val_loss: 0.5705 - val_accuracy: 0.7854\nEpoch 28/30\n98795/98795 - 10s - loss: 0.4906 - accuracy: 0.8149 - val_loss: 0.5626 - val_accuracy: 0.7907\nEpoch 29/30\n98795/98795 - 10s - loss: 0.4870 - accuracy: 0.8155 - val_loss: 0.5775 - val_accuracy: 0.7822\nEpoch 30/30\n98795/98795 - 11s - loss: 0.4823 - accuracy: 0.8175 - val_loss: 0.5710 - val_accuracy: 0.7879\n"
    }
   ],
   "source": [
    "model_cnn =  model.fit(train_a, train_y, batch_size=64, epochs=30, validation_data=(dev_a, dev_y), verbose=2,callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_5 (Embedding)      (None, 10, 100)           737400    \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 10, 100)           0         \n_________________________________________________________________\nconv1d (Conv1D)              (None, 6, 64)             32064     \n_________________________________________________________________\nmax_pooling1d (MaxPooling1D) (None, 1, 64)             0         \n_________________________________________________________________\nlstm_4 (LSTM)                (None, 100)               66000     \n_________________________________________________________________\ndense_10 (Dense)             (None, 5)                 505       \n=================================================================\nTotal params: 835,969\nTrainable params: 98,569\nNon-trainable params: 737,400\n_________________________________________________________________\n"
    }
   ],
   "source": [
    "lstm_cnn = Sequential()\n",
    "lstm_cnn.add(Embedding(vocab_size+1, 100,weights=[embedding_matrix_lp],input_length=windows_size, trainable=False))\n",
    "lstm_cnn.add(Dropout(0.2))\n",
    "lstm_cnn.add(Conv1D(64, 5, activation='relu'))\n",
    "lstm_cnn.add(MaxPooling1D(pool_size=4))\n",
    "lstm_cnn.add(LSTM(100))\n",
    "lstm_cnn.add(Dense(5, activation='softmax'))\n",
    "lstm_cnn.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "## Fit train data\n",
    "lstm_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Train on 64316 samples, validate on 12864 samples\nEpoch 1/30\n64316/64316 [==============================] - 8s 132us/sample - loss: 1.5772 - accuracy: 0.2717 - val_loss: 1.5561 - val_accuracy: 0.2950\nEpoch 2/30\n64316/64316 [==============================] - 7s 109us/sample - loss: 1.5218 - accuracy: 0.3209 - val_loss: 1.5106 - val_accuracy: 0.3305\nEpoch 3/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.4552 - accuracy: 0.3693 - val_loss: 1.4500 - val_accuracy: 0.3727\nEpoch 4/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.4037 - accuracy: 0.3996 - val_loss: 1.4132 - val_accuracy: 0.4007\nEpoch 5/30\n64316/64316 [==============================] - 7s 110us/sample - loss: 1.3633 - accuracy: 0.4226 - val_loss: 1.3710 - val_accuracy: 0.4261\nEpoch 6/30\n64316/64316 [==============================] - 7s 110us/sample - loss: 1.3329 - accuracy: 0.4396 - val_loss: 1.3698 - val_accuracy: 0.4250\nEpoch 7/30\n64316/64316 [==============================] - 7s 111us/sample - loss: 1.3079 - accuracy: 0.4503 - val_loss: 1.3298 - val_accuracy: 0.4503\nEpoch 8/30\n64316/64316 [==============================] - 7s 110us/sample - loss: 1.2850 - accuracy: 0.4657 - val_loss: 1.3139 - val_accuracy: 0.4542\nEpoch 9/30\n64316/64316 [==============================] - 7s 110us/sample - loss: 1.2728 - accuracy: 0.4695 - val_loss: 1.2936 - val_accuracy: 0.4647\nEpoch 10/30\n64316/64316 [==============================] - 7s 111us/sample - loss: 1.2589 - accuracy: 0.4782 - val_loss: 1.2824 - val_accuracy: 0.4711\nEpoch 11/30\n64316/64316 [==============================] - 7s 115us/sample - loss: 1.2473 - accuracy: 0.4836 - val_loss: 1.2756 - val_accuracy: 0.4773\nEpoch 12/30\n64316/64316 [==============================] - 7s 111us/sample - loss: 1.2398 - accuracy: 0.4848 - val_loss: 1.2704 - val_accuracy: 0.4767\nEpoch 13/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.2272 - accuracy: 0.4950 - val_loss: 1.2658 - val_accuracy: 0.4799\nEpoch 14/30\n64316/64316 [==============================] - 7s 107us/sample - loss: 1.2156 - accuracy: 0.5000 - val_loss: 1.2534 - val_accuracy: 0.4897\nEpoch 15/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.2116 - accuracy: 0.4992 - val_loss: 1.2579 - val_accuracy: 0.4845\nEpoch 16/30\n64316/64316 [==============================] - 7s 107us/sample - loss: 1.2003 - accuracy: 0.5079 - val_loss: 1.2499 - val_accuracy: 0.4881\nEpoch 17/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.1968 - accuracy: 0.5093 - val_loss: 1.2334 - val_accuracy: 0.4937\nEpoch 18/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.1850 - accuracy: 0.5140 - val_loss: 1.2404 - val_accuracy: 0.4904\nEpoch 19/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.1812 - accuracy: 0.5166 - val_loss: 1.2271 - val_accuracy: 0.5026\nEpoch 20/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.1776 - accuracy: 0.5155 - val_loss: 1.2213 - val_accuracy: 0.5018\nEpoch 21/30\n64316/64316 [==============================] - 7s 109us/sample - loss: 1.1758 - accuracy: 0.5199 - val_loss: 1.2183 - val_accuracy: 0.5048\nEpoch 22/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.1672 - accuracy: 0.5241 - val_loss: 1.2152 - val_accuracy: 0.5044\nEpoch 23/30\n64316/64316 [==============================] - 7s 107us/sample - loss: 1.1657 - accuracy: 0.5236 - val_loss: 1.2127 - val_accuracy: 0.5051\nEpoch 24/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.1583 - accuracy: 0.5272 - val_loss: 1.2048 - val_accuracy: 0.5159\nEpoch 25/30\n64316/64316 [==============================] - 7s 107us/sample - loss: 1.1554 - accuracy: 0.5285 - val_loss: 1.2006 - val_accuracy: 0.5150\nEpoch 26/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.1537 - accuracy: 0.5280 - val_loss: 1.1914 - val_accuracy: 0.5194\nEpoch 27/30\n64316/64316 [==============================] - 7s 107us/sample - loss: 1.1512 - accuracy: 0.5323 - val_loss: 1.1994 - val_accuracy: 0.5104\nEpoch 28/30\n64316/64316 [==============================] - 7s 108us/sample - loss: 1.1429 - accuracy: 0.5361 - val_loss: 1.1977 - val_accuracy: 0.5130\nEpoch 29/30\n64316/64316 [==============================] - 7s 106us/sample - loss: 1.1459 - accuracy: 0.5344 - val_loss: 1.2013 - val_accuracy: 0.5113\n"
    }
   ],
   "source": [
    "hist = lstm_cnn.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_cnn.save('model_lstm_cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        \n",
    "        self.W = self.add_weight(name='Attention_Weight',\n",
    "                                 shape=(input_shape[-1], input_shape[-1]),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(name='Attention_Bias',\n",
    "                                 shape=(input_shape[-1], ),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        self.u = self.add_weight(name='Attention_Context_Vector',\n",
    "                                 shape=(input_shape[-1], 1),\n",
    "                                 initializer=self.init,\n",
    "                                 trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "    \n",
    "    def call(self, x):\n",
    "        # refer to the original paper\n",
    "        # link: https://www.cs.cmu.edu/~hovy/papers/16HLT-hierarchical-attention-networks.pdf\n",
    "        \n",
    "        u_it = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        \n",
    "       #Through structure the vector applied  is used as\n",
    "        #Make attention value into probability distribution through\n",
    "        a_it = K.dot(u_it, self.u)\n",
    "        a_it = K.squeeze(a_it, -1)\n",
    "        a_it = K.softmax(a_it)\n",
    "        \n",
    "        return a_it\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], input_shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_inp = Input(shape=(windows_size, ))\n",
    "embedding_size_glove = 100\n",
    "answer_emb1 = Embedding(vocab_size+1, embedding_size_glove, weights=[embedding_matrix_lp], input_length=windows_size, trainable=False)(answer_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_dim=50\n",
    "bilstm = Bidirectional(LSTM(lstm_dim, return_sequences=True))(answer_emb1)  #Input should be three dimensional\n",
    "# bilstm = LSTM(2*lstm_dim, return_sequences=True)(input_data) \n",
    "# bilstm_output = Dense(1)(bilstm)\n",
    "\n",
    "attention_layer = AttentionLayer()(bilstm)\n",
    "print(attention_layer)\n",
    "repeated_word_attention = RepeatVector(lstm_dim * 2)(attention_layer)\n",
    "repeated_word_attention = Permute([2, 1])(repeated_word_attention)\n",
    "sentence_representation = Multiply()([bilstm, repeated_word_attention])\n",
    "sentence_representation = Lambda(lambda x: K.sum(x, axis=1))(sentence_representation) #total summation of the multiplied bilstm and attention \n",
    "\n",
    "bilstm_output = Dense(5,activation='softmax')(sentence_representation)\n",
    "\n",
    "model = Model(inputs=[answer_inp],\n",
    "            outputs=[bilstm_output])\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_a_b, train_y_b, validation_data=(dev_a_b, dev_y_b), epochs=30, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_bilstm_attn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}